---
title: "Naming and Locating Objects in Images | TF4R Blog <img src=\"Keras_Logo.jpg\" style=\"float: right; width: 80px;\"/>"
author: "Michael Rose"
output: 
  html_document:
     highlight: zenburn
     theme: lumen
     df_print: paged
     fig_align: center
     code_folding: hide
---

```{r, echo = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, out.width = "100%")

# programming
library(tensorflow)
library(keras)
```

# {.tabset}

## Intro

This is a work-through of the tutorial at 
[Naming and Locating Objects in Images](https://blogs.rstudio.com/tensorflow/posts/2018-11-05-naming-locating-objects/)


### Check GPU Availability

```{r}
K = backend()
sess = K$get_session()
sess$list_devices()
```

## Data 

We will be using images and annotations from the Pascal VOC dataset. Specifically, we will use the data from the 2007 challenge and the same JSON annotation file. 

```{r, eval = FALSE}
# mkdir data && cd data
# curl -OL http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar
# curl -OL https://storage.googleapis.com/coco-dataset/external/PASCAL_VOC.zip
# tar -xf VOCtrainval_06-Nov-2007.tar
# unzip PASCAL_VOC.zip
# mv PASCAL_VOC/*.json .
# rmdir PASCAL_VOC
# tar -xvf VOCtrainval_06-Nov-2007.tar
```

### Set up directories 

```{r}
img_dir <- "data/VOCdevkit/VOC2007/JPEGImages"
annot_file <- "data/pascal_train2007.json"
```

Now we need to extract some information from that json file. 

### Preprocessing 

```{r}
# load libraries
library(rjson)
library(magick)
library(tidyverse)
library(magrittr)
```

The annotations contain information about three types of things we are interested in. 

```{r}
annotations <- fromJSON(file = annot_file)
str(annotations, max.level = 1)
```

First we have characteristics of the image itself (height and width) and where its stored. 

```{r}
imageinfo <- annotations$images %>% {
    tibble(
        id = map_dbl(., "id"),
        file_name = map_chr(., "file_name"),
        image_height = map_dbl(., "height"),
        image_width = map_dbl(., "width")
    )
}
```

In PascalVOC there are 20 object classes. 

```{r}
classes <- c("aeroplane",
             "bicycle",
             "bird",
             "boat",
             "bottle",
             "bus",
             "car",
             "cat",
             "chair",
             "cow",
             "diningtable",
             "dog",
             "horse",
             "motorbike",
             "person",
             "pottedplant",
             "sheep",
             "sofa",
             "train",
             "tvmonitor")

boxinfo <- annotations$annotations %>% {
    tibble(
        image_id = map_dbl(., "image_id"),
        category_id = map_dbl(., "category_id"),
        bbox = map(., "bbox")
    )
}

# unpack bounding boxes from their list column
boxinfo %<>% mutate(bbox = unlist(map(.$bbox, function(x) paste(x, collapse = " "))))

boxinfo %<>% separate(bbox, into = c("x_left", "y_top",
                                     "bbox_width", "bbox_height")) 

boxinfo %<>% mutate_all(as.numeric) 
```

For bounding boxes, the annotation file provides `x_left` and `y_top` coordinates, as well as width and height. We will mostly be working with corner coordinates, so we can create the missing `x_right` and `y_bottom`.

As is usual in image processing, the y axis starts from the top

```{r}
# add other coordinates
boxinfo %<>% mutate(y_bottom = y_top + bbox_height - 1,
                    x_right = x_left + bbox_width - 1) 

# match class ids to class names
catinfo <- annotations$categories %>% {
    tibble(id = map_dbl(., "id"), name = map_chr(., "name"))
}

# all together now
imageinfo %<>%
    inner_join(boxinfo, by = c("id" = "image_id")) %>%
    inner_join(catinfo, by = c("category_id" = "id"))
```

Note that we still have several entries per image, each annotated object occupying its own row. 

In order to avoid hurting our localization performance down the road, we need to scale all bounding box coordinates according to the actual image size we'll use when we pass it to our network.

```{r}
target_height <- 224
target_width <- 224

imageinfo %<>% mutate(x_left_scaled = (x_left / image_width * target_width) %>% round(),
                      x_right_scaled = (x_right / image_width * target_width) %>% round(),
                      y_top_scaled = (y_top / image_height * target_height) %>% round(),
                      y_bottom_scaled = (y_bottom / image_height * target_height) %>% round(),
                      bbox_width_scaled =  (bbox_width / image_width * target_width) %>% round(),
                      bbox_height_scaled = (bbox_height / image_height * target_height) %>% round()) 
```

Let's take a look at our data. Picking one of the early entries and displaying the original image together with the object annotation yields: 

```{r}
img_data <- imageinfo[4,]
img <- image_read(file.path(img_dir, img_data$file_name))
img <- image_draw(img)
image_display(img)
rect(
    img_data$x_left,
    img_data$x_right,
    img_data$y_top,
    img_data$y_bottom,
    border = "purple",
    lwd = 2
)
text(
    img_data$x_left,
    img_data$y_top,
    img_data$name,
    offset = 1,
    pos = 2,
    cex = 1.5,
    col = "purple"
)
dev.off()
```

In this tutorial, we will be focusing on handling a single object in an image. This means that we must decide which object to single out for each image. 

A reasonable strategy is to choose the object with the largest ground truth bounding box.

```{r}
imageinfo %<>% mutate(area = bbox_width_scaled * bbox_height_scaled) 

imageinfo_maxbb <- imageinfo %>%
    group_by(id) %>%
    filter(which.max(area) == row_number())
```

After this operation we only have 2501 images to work with (not many at all). For classification, we could use data augmentation as provided by Keras, but to work with localization we would need to spin our own augmentation algorithm. We will focus on just the basics for now. 

```{r}
# set up train test splits
n_samples <- nrow(imageinfo)
train_indices <- sample(1:n_samples, 0.8 * n_samples)
train_data <- imageinfo_maxbb[train_indices, ]
validation_data <- imageinfo_maxbb[-train_indices,]
```

## Single Object 

In all cases we will use Xception as a basic feature extractor. Since it was trained on imagenet, we don't expect much fine tuning to be necessary to adapt to Pascal VOC - so we will leave its weights untouched.

```{r}
feature_extractor <- application_xception(
    include_top = FALSE,
    input_shape = c(224, 224, 3),
    pooling = "avg"
)

# freeze weights to keep imagenet weighting
feature_extractor %>% freeze_weights()

# put some custom layers on top
model <- keras_model_sequential() %>%
    feature_extractor() %>%
    layer_batch_normalization() %>%
    layer_dropout(rate = 0.25) %>%
    layer_dense(units = 512, activation = "relu") %>%
    layer_batch_normalization() %>%
    layer_dropout(rate = 0.5) %>%
    layer_dense(units = 20, activation = "softmax")

model %>% compile(
              optimizer = "adam",
              loss = "sparse_categorical_crossentropy",
              metrics = list("accuracy")
          )
```

To pass our data to Keras, we could use `image_data_generator`. Soon we will need custom generators though, so we will build a simple one ourselves. This one delivers images as well as the corresponding targets in a stream. Note how the targets are not onehot encoded, but integers. Our use of `sparse_categorical_crossentropy` as a loss function enables this convenience. 

```{r}
batch_size <- 10

load_and_preprocess_image <- function(image_name, target_height, target_width) {
  img_array <- image_load(
    file.path(img_dir, image_name),
    target_size = c(target_height, target_width)
    ) %>%
    image_to_array() %>%
    xception_preprocess_input() 
  dim(img_array) <- c(1, dim(img_array))
  img_array
}

classification_generator <-
  function(data,
           target_height,
           target_width,
           shuffle,
           batch_size) {
    i <- 1
    function() {
      if (shuffle) {
        indices <- sample(1:nrow(data), size = batch_size)
      } else {
        if (i + batch_size >= nrow(data))
          i <<- 1
        indices <- c(i:min(i + batch_size - 1, nrow(data)))
        i <<- i + length(indices)
      }
      x <-
        array(0, dim = c(length(indices), target_height, target_width, 3))
      y <- array(0, dim = c(length(indices), 1))
      
      for (j in 1:length(indices)) {
        x[j, , , ] <-
          load_and_preprocess_image(data[[indices[j], "file_name"]],
                                    target_height, target_width)
        y[j, ] <-
          data[[indices[j], "category_id"]] - 1
      }
      x <- x / 255
      list(x, y)
    }
  }

train_gen <- classification_generator(
  train_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = TRUE,
  batch_size = batch_size
)

valid_gen <- classification_generator(
  validation_data,
  target_height = target_height,
  target_width = target_width,
  shuffle = FALSE,
  batch_size = batch_size
)

model %>% fit_generator(
  train_gen,
  epochs = 20,
  steps_per_epoch = nrow(train_data) / batch_size,
  validation_data = valid_gen,
  validation_steps = nrow(validation_data) / batch_size,
  callbacks = list(
    callback_model_checkpoint(
      file.path("class_only", "weights.{epoch:02d}-{val_loss:.2f}.hdf5")
    ),
    callback_early_stopping(patience = 2)
  )
)
```
