---
title: "Auto Keras | TF4R Blog <img src=\"Keras_Logo.jpg\" style=\"float: right; width: 80px;\"/>"
author: "Michael Rose"
output: 
  html_document:
     highlight: zenburn
     theme: lumen
     df_print: paged
     fig_align: center
     code_folding: hide
---

```{r, echo = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, out.width = "100%")

# programming
library(tensorflow)
library(keras)
```

# {.tabset}

## Intro

This is a work-through of the tutorial at [autokeras](https://blogs.rstudio.com/tensorflow/posts/2019-04-16-autokeras/).

Autokeras uses the research of Neural Architecture Search. Here are some of the papers on it: 

- Designing neural network architectures using reinforcement learning
  - B. Baker, O. Gupta, N. Naik, R. Raskar.
  - arXiv preprint arXiv:1611.02167. 2016. 
  - [PDF](https://arxiv.org/pdf/1611.02167.pdf)
  - Abstract: 
  
```
At present, designing convolutional neural network (CNN)
architectures requires both human expertise and labor. New
architectures are handcrafted by careful experimentation or modified
from a handful of existing networks. We introduce MetaQNN, a
meta-modeling algorithm based on reinforcement learning to
automatically generate high-performing CNN architectures for a given
learning task. The learning agent is trained to sequentially choose
CNN layers using Qlearning with an -greedy exploration strategy and
experience replay. The agent explores a large but finite space of
possible architectures and iteratively discovers designs with improved
performance on the learning task. On image classification benchmarks,
the agent-designed networks (consisting of only standard convolution,
pooling, and fully-connected layers) beat existing networks designed
with the same layer types and are competitive against the
state-of-the-art methods that use more complex layer types. We also
outperform existing meta-modeling approaches for network design on
image classification tasks.
```

- Efficient neural architecture search via parameter sharing 
  - H. Pham, M.Y. Guan, B. Zoph, Q.V. Le, J. Dean.
  - arXiv preprint arXiv:1802.03268. 2018.  
  - [PDF](https://arxiv.org/pdf/1802.03268.pdf)
  - Abstract: 

```  
We propose Efficient Neural Architecture Search (ENAS), a fast and
inexpensive approach for automatic model design. In ENAS, a controller
discovers neural network architectures by searching for an optimal
subgraph within a large computational graph. The controller is trained
with policy gradient to select a subgraph that maximizes the expected
reward on a validation set. Meanwhile the model corresponding to the
selected subgraph is trained to minimize a canonical cross entropy
loss. Sharing parameters among child models allows ENAS to deliver
strong empirical performances, while using much fewer GPUhours than
existing automatic model design approaches, and notably, 1000x less
expensive than standard Neural Architecture Search. On the Penn
Treebank dataset, ENAS discovers a novel architecture that achieves a
test perplexity of 55.8, establishing a new state-of-the-art among all
methods without post-training processing. On the CIFAR-10 dataset,
ENAS finds a novel architecture that achieves 2.89% test error, which
is on par with the 2.65% test error of NASNet (Zoph et al., 2018).
```

- Neural architecture search with reinforcement learning
  - B. Zoph, Q.V. Le.
  - arXiv preprint arXiv:1611.01578. 2016.  
  - [PDF](https://arxiv.org/pdf/1611.01578.pdf)
  - Abstract: 

```
Neural networks are powerful and flexible models that work well for
many difficult learning tasks in image, speech and natural language
understanding. Despite their success, neural networks are still hard
to design. In this paper, we use a recurrent network to generate the
model descriptions of neural networks and train this RNN with
reinforcement learning to maximize the expected accuracy of the
generated architectures on a validation set. On the CIFAR-10 dataset,
our method, starting from scratch, can design a novel network
architecture that rivals the best human-invented architecture in terms
of test set accuracy. Our CIFAR-10 model achieves a test error rate of
3.65, which is 0.09 percent better and 1.05x faster than the previous
state-of-the-art model that used a similar architectural scheme. On
the Penn Treebank dataset, our model can compose a novel recurrent
cell that outperforms the widely-used LSTM cell, and other
state-of-the-art baselines. Our cell achieves a test set perplexity of
62.4 on the Penn Treebank, which is 3.6 perplexity better than the
previous state-of-the-art model. The cell can also be transferred to
the character language modeling task on PTB and achieves a
state-of-the-art perplexity of 1.214.
```
  
- Neural architecture optimization 
  - R. Luo, F. Tian, T. Qin, E. Chen, T. Liu.  
  - Advances in neural information processing systems, pp. 7816--7827. 2018.
  - [PDF](https://arxiv.org/pdf/1808.07233.pdf)
  - Abstract: 
  
```
Automatic neural architecture design has shown its potential in
discovering powerful neural network architectures. Existing methods,
no matter based on reinforcement learning or evolutionary algorithms
(EA), conduct architecture search in a discrete space, which is highly
inefficient. In this paper, we propose a simple and efficient method
to automatic neural architecture design based on continuous
optimization. We call this new approach neural architecture
optimization (NAO). There are three key components in our proposed
approach: (1) An encoder embeds/maps neural network architectures into
a continuous space. (2) A predictor takes the continuous
representation of a network as input and predicts its accuracy. (3) A
decoder maps a continuous representation of a network back to its
architecture.  The performance predictor and the encoder enable us to
perform gradient based optimization in the continuous space to find
the embedding of a new architecture with potentially better
accuracy. Such a better embedding is then decoded to a network by the
decoder. Experiments show that the architecture discovered by our
method is very competitive for image classification task on CIFAR-10
and language modeling task on PTB, outperforming or on par with the
best results of previous architecture search methods with a
significantly reduction of computational resources. Specifically we
obtain 2.11% test set error rate for CIFAR-10 image classification
task and 56.0 test set perplexity of PTB language modeling task. The
best discovered architectures on both tasks are successfully
transferred to other tasks such as CIFAR-100 and
WikiText-2. Furthermore, combined with the recent proposed weight
sharing mechanism, we discover powerful architecture on CIFAR-10 (with
error rate 3.53%) and on PTB (with test set perplexity 56.6), with
very limited computational resources (less than 10 GPU hours) for both
tasks.
```

- Hierarchical representations for efficient architecture search 
  - H. Liu, K. Simonyan, O. Vinyals, C. Fernando, K. Kavukcuoglu.  
  - arXiv preprint arXiv:1711.00436. 2017.  
  - [PDF](https://arxiv.org/pdf/1711.00436.pdf)
  - Abstract:

```
We explore efficient neural architecture search methods and show that
a simple yet powerful evolutionary algorithm can discover new
architectures with excellent performance. Our approach combines a
novel hierarchical genetic representation scheme that imitates the
modularized design pattern commonly adopted by human experts, and an
expressive search space that supports complex topologies. Our
algorithm efficiently discovers architectures that outperform a large
number of manually designed models for image classification, obtaining
top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to
ImageNet, which is competitive with the best existing neural
architecture search approaches. We also present results using random
search, achieving 0.3% less top-1 accuracy on CIFAR-10 and 0.1% less
on ImageNet whilst reducing the search time from 36 hours down to 1
hour.
```

- Regularized evolution for image classifier architecture search 
  - E. Real, A. Aggarwal, Y. Huang, Q.V. Le.  
  - arXiv preprint arXiv:1802.01548. 2018.  
  - [PDF](https://arxiv.org/pdf/1802.01548.pdf)
  - Abstract: 
  
```
The effort devoted to hand-crafting neural network image classifiers
has motivated the use of architecture search to discover them
automatically. Although evolutionary algorithms have been repeatedly
applied to neural network topologies, the image classifiers thus
discovered have remained inferior to human-crafted ones. Here, we
evolve an image classifierâ€” AmoebaNet-Aâ€”that surpasses hand-designs
for the first time.  To do this, we modify the tournament selection
evolutionary algorithm by introducing an age property to favor the
younger genotypes. Matching size, AmoebaNet-A has comparable accuracy
to current state-of-the-art ImageNet models discovered with more
complex architecture-search methods.  Scaled to larger size,
AmoebaNet-A sets a new state-of-theart 83.9% top-1 / 96.6% top-5
ImageNet accuracy. In a controlled comparison against a well known
reinforcement learning algorithm, we give evidence that evolution can
obtain results faster with the same hardware, especially at the
earlier stages of the search. This is relevant when fewer compute
resources are available. Evolution is, thus, a simple method to
effectively discover high-quality architectures.
```  

- Auto-Keras: An Efficient Neural Architecture Search System 
  - H. Jin, Q. Song, X. Hu.  
  - arXiv preprint arXiv:1806.10282. 2018.
  - [PDF](https://arxiv.org/pdf/1806.10282.pdf)
  - Abstract:
  
```
Neural architecture search (NAS) has been proposed to automatically
tune deep neural networks, but existing search algorithms, e.g.,
NASNet [41], PNAS [22], usually suffer from expensive computational
cost. Network morphism, which keeps the functionality of a neural
network while changing its neural architecture, could be helpful for
NAS by enabling more efficient training during the search. In this
paper, we propose a novel framework enabling Bayesian optimization to
guide the network morphism for efficient neural architecture
search. The framework develops a neural network kernel and a
tree-structured acquisition function optimization algorithm to
efficiently explores the search space. Intensive experiments on
real-world benchmark datasets have been done to demonstrate the
superior performance of the developed framework over the
state-of-the-art methods. Moreover, we build an opensource AutoML
system based on our method, namely Auto-Keras.1 The system runs in
parallel on CPU and GPU, with an adaptive search strategy for
different GPU memory limits.
```

### Check GPU Availability

```{r}
K = backend()
sess = K$get_session()
sess$list_devices()

# devtools::install_github("jcrodriguez1989/autokeras", dependencies = TRUE, force = TRUE)

library("autokeras")
```

### Examples 

#### CIFAR10 Dataset 

```{r}
# get cifar10 dataset
# cifar10 <- dataset_cifar10()
# c(x_train, y_train) %<-% cifar10$train
# c(x_test, y_test) %<-% cifar10$test 
 
# create an image classifier and train different models for 3 minutes
## clf <- model_image_classifier(verbose = TRUE, augment = FALSE) %>%
##     fit(x_train, y_train, time_limit = 3 * 60)
```

I can't get this to run right now. See the blog post for the rest. 

Currently autokeras only runs on python 3.6 and tensorflow 1.12.0. 
