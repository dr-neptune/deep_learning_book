---
title: "Keras Tutorials <img src=\"Keras_Logo.jpg\" style=\"float: right; width: 80px;\"/>"
author: "Michael Rose"
output: 
  html_document:
     highlight: zenburn
     theme: lumen
     df_print: paged
     fig_align: center
     code_folding: hide
---

```{r, echo = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

# programming 
library(tensorflow)
library(keras)
```

# {.tabset}

## Intro

This is a work-through of the tutorials on the [Keras Homepage for R](https://keras.rstudio.com/index.html).

## Getting Started 

### Check GPU Availability

```{r}
K = backend()
sess = K$get_session()
sess$list_devices()
```

### MNIST Example 

#### Preparing the Data

```{r}
# load data
mnist <- dataset_mnist()

x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y

# reshape into a single dimension
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))

# rescale
x_train <- x_train / 255
x_test <- x_test / 255

# prepare labels for one hot encoding
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
```

#### Defining the Model 

```{r}
# create sequential model
model <- keras_model_sequential()

# input length 784 numeric vector -> output length 10 numeric vector
model %>%
    layer_dense(units = 256, activation = "relu", input_shape = c(784)) %>%
    layer_dropout(rate = 0.4) %>%
    layer_dense(units = 128, activation = "relu") %>%
    layer_dropout(rate = 0.3) %>%
    layer_dense(units = 10, activation = "softmax")

# look at model details
model %>% summary()

# compile the model with loss function, optimizer, metrics
model %>% compile(
              loss = 'categorical_crossentropy',
              optimizer = optimizer_rmsprop(),
              metrics = c('accuracy')
)
```

#### Training and Evaluation

```{r}
# use fit to train model for 30 epochs using 128 image batches
history <- model %>% fit(
                         x_train, y_train,
                         epochs = 30,
                         batch_size = 128,
                         validation_split = 0.2
                     )

# plot
plot(history)
```

```{r}
# evaluate model on the test set
model %>% evaluate(x_test, y_test)

# generate predictions on new data
model %>% predict_classes(x_test) %>% head()
```

## Basic Classification

### Fashion MNIST

```{r}
# load data
fashion_mnist <- dataset_fashion_mnist()

# create CV sets
c(train_images, train_labels) %<-% fashion_mnist$train
c(test_images, test_labels) %<-% fashion_mnist$test 

# define class names
class_names <- c('T-shirt/top',
                'Trouser',
                'Pullover',
                'Dress',
                'Coat', 
                'Sandal',
                'Shirt',
                'Sneaker',
                'Bag',
                'Ankle boot')

```

#### Explore the Data

```{r}
train_images %>% dim()
test_images %>% dim()

train_labels[1:20]
test_labels[1:20]
```

#### Preprocess the Data

```{r}
library(tidyr)
library(ggplot2)

image_1 <- as.data.frame(train_images[1, , ])
colnames(image_1) <- seq_len(ncol(image_1))
image_1$y <- seq_len(nrow(image_1))
image_1 <- gather(image_1, "x", "value", -y)
image_1$x <- as.integer(image_1$x)

image_1 %>% 
    ggplot(aes(x = x, y = y, fill = value)) +
    geom_tile() +
    scale_fill_gradient(low = "white", high = "black", na.value = NA) +
    scale_y_reverse() +
    theme_minimal() +
    theme(panel.grid = element_blank()) +
    theme(aspect.ratio = 1) +
    xlab("") + ylab("")
```

We scale these values to a range of 0 to 1 before feeding it to the neural network model. For this we divide by 255. It is important that the training and test sets are preprocessed in the same way. 

```{r}
# scale
train_images <- train_images / 255
test_images <- test_images / 255

# look at data
par(mfcol = c(5,5))
par(mar = c(0, 0, 1.5, 0), xaxs = 'i', yaxs = 'i')
for (i in 1:25) {
    img <- train_images[i, , ]
    img <- t(apply(img, 2, rev))
    image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',
          main = paste(class_names[train_labels[i] + 1]))
    }

```

#### Build the Model 

```{r}
model <- keras_model_sequential()

model %>%
    # transform 28x28 -> 784
    layer_flatten(input_shape = c(28, 28)) %>%
    layer_dense(units = 128, activation = "relu") %>%
    layer_dense(units = 10, activation = "softmax")

# compile
model %>% compile(
              optimizer = "adam",
              loss = "sparse_categorical_crossentropy",
              metrics = c("accuracy")
)

# train the model
history <- model %>% fit(train_images, train_labels, epochs = 15)

plot(history)
```

#### Evaluate Accuracy

```{r}
# eval model on test set
score <- model %>% evaluate(test_images, test_labels)
cat("Test Loss:", score$loss, "\n")
cat("Test Accuracy:", score$acc, "\n")

# make predictions
predictions <- model %>% predict(test_images)
predictions[1, ]

# get best prediction from softmax array
which.max(predictions[1, ])

# directly get class prediction
class_pred <- model %>% predict_classes(test_images)
class_pred[1:20]

# check if label is correct
test_labels[9]
```

Lets plot some images with their predictions. Correct prediction labels are in green and incorrect prediction labels are in red. 

```{r}
par(mfcol=c(5,5))
par(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')
for (i in 1:25) { 
  img <- test_images[i, , ]
  img <- t(apply(img, 2, rev)) 
  # subtract 1 as labels go from 0 to 9
  predicted_label <- which.max(predictions[i, ]) - 1
  true_label <- test_labels[i]
  if (predicted_label == true_label) {
    color <- '#008800' 
  } else {
    color <- '#bb0000'
  }
  image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',
        main = paste0(class_names[predicted_label + 1], " (",
                      class_names[true_label + 1], ")"),
        col.main = color)
}
```

Finally, use the trained model to make a prediction about a single image: 

```{r}
img <- test_images[1, , , drop = FALSE]
img %>% dim()

# predict image
(predictions <- model %>% predict(img))
```

`predict` returns a list of lists, one for each image in the batch of data. Grab the predictions for our only image in the batch

```{r}
# subtract 1 as labels are 0-based
prediction <- predictions[1, ] - 1
which.max(prediction)
```

## Text Classification 

This tutorial classifies movie reviews as positive or negative. 

```{r}
library(dplyr)
library(purrr)
```

### IMDB

#### Get Dataset

```{r}
# load data, max 10k words / review
imdb <- dataset_imdb(num_words = 10000)
c(train_data, train_labels) %<-% imdb$train
c(test_data, test_labels) %<-% imdb$test 

# index mapping words to integers
word_index <- dataset_imdb_word_index()
```

#### Explore the Data 

The data comes preprocessed; each example is an array of integers representing the words of the movie review. Each label is an integer value of either 0 or 1, where 0 is a negative review and 1 is a positive review.

```{r}
paste0("Training Entries: ", length(train_data), ", Labels: ", length(train_labels))

# look at encoded text
train_data[[1]]
```

Movie reviews may be different lengths. The code below shows the number of words in the first and second reviews. Since inputs to a neural network must be the same length, we need to resolve this later. 

```{r}
length(train_data[[1]])
length(train_data[[2]])
```

Lets convert the integers back to text to get a look at some of the data. 

```{r}
word_index_df <- data.frame(
  word = names(word_index),
  idx = unlist(word_index, use.names = FALSE),
  stringsAsFactors = FALSE
)

# The first indices are reserved  
word_index_df <- word_index_df %>% mutate(idx = idx + 3)
word_index_df <- word_index_df %>%
  add_row(word = "<PAD>", idx = 0)%>%
  add_row(word = "<START>", idx = 1)%>%
  add_row(word = "<UNK>", idx = 2)%>%
  add_row(word = "<UNUSED>", idx = 3)

word_index_df <- word_index_df %>% arrange(idx)

decode_review <- function(text){
  paste(map(text, function(number) word_index_df %>%
              filter(idx == number) %>%
              select(word) %>% 
              pull()),
        collapse = " ")
}


# display text for first review
decode_review(train_data[[1]])
```

#### Prepare the Data 




## Basic Regression

## Overfitting / Underfitting 

## Save / Restore Models 

