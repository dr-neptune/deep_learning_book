---
title: "Keras Articles <img src=\"Keras_Logo.jpg\" style=\"float: right; width: 80px;\"/>"
author: "Michael Rose"
output: 
  html_document:
     highlight: zenburn
     theme: lumen
     df_print: paged
     fig_align: center
     code_folding: hide
---

```{r, echo = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, out.width = "100%")

# programming 
library(tensorflow)
library(keras)
```

# {.tabset}

## Intro

This is a work-through of the tutorials on the [Keras Homepage for R](https://keras.rstudio.com/index.html).

## Basics 
 
### Check GPU Availability

```{r}
K = backend()
sess = K$get_session()
sess$list_devices()
```

#### Build a Simple Model 

```{r}
# build a simple fully connected network
model <- keras_model_sequential()

model %>%
    layer_dense(units = 64, activation = "relu") %>%
    layer_dense(units = 64, activation = "relu") %>%
    layer_dense(units = 10, activation = "softmax")
```

#### Configure the Layers

There are many layers available with some common constructor parameters: 

- `activation`: set the activation function for the layer. 
- `kernel_initializer`, `bias_initializer`: create the layers initial weights (kernel and bias). This defaults to the `Glorot uniform` initializer. 
- `kernel_regularizer`, `bias_regularizer`: The regularization schemes that apply to the layer's weights (kernels and bias), such as L1 or L2 regularization. By default, no regularization is applied. 

The following instantiates `dense` layers using constructor arguments.

```{r}
# sigmoid layer
layer_dense(units = 64, activation = "sigmoid")

# linear layer with L1 regularization of factor 0.01 applied to the kernel matrix
layer_dense(units = 64, kernel_regularizer = regularizer_l1(0.01))

# a linear layer with L2 regularization of factor 0.01 applied to the bias vector
layer_dense(units = 64, bias_regularizer = regularizer_l2(0.01))

# a linear layer with a bias vector initialized to 2.0
layer_dense(units = 64, bias_initializer = initializer_constant(2.0))
```

#### Train and Evaluate 

```{r}
# compile
model %>% compile(
              optimizer = "adam",
              loss = "categorical_crossentropy",
              metrics = list("accuracy")
          )
```

`compile` takes 3 important arguments: 

- `optimizer` : This object specifies the training procedure. [Overview of Gradient Descent Algorithms](http://ruder.io/optimizing-gradient-descent/index.html)

- `loss`: The function to minimize during optimization. Common choices include mean squared error for regression, categorical_crossentropy for multiclass classification, and binary_crossentropy for binary classification. 

- `metrics`: Used to monitor training. In classification, this is usually accuracy. 

```{r}
# configure a model for mean-squared error regression
model %>% compile(
              optimizer = "adam",
              loss = "mse",
              metrics = list("mae")
          )

# configure a model for categorical classification
model %>% compile(
              optimizer = optimizer_rmsprop(lr = 0.01),
              loss = "categorical_crossentropy",
              metrics = list("categorical_accuracy")
)
```

#### Input Data

We can train keras models directly on R matrices and arrays. A model is fit to the training data using the fit method. 

```{r}
data <- matrix(rnorm(1000 * 32), nrow = 1000, ncol = 32)
labels <- matrix(rnorm(1000 * 10), nrow = 1000, ncol = 10)

model %>% fit(
              data,
              labels,
              epochs = 10,
              batch_size = 32
          )
```

`fit` takes three important arguments: 

- `epochs` : Training is structured into epochs. An epoch is one iteration over the entire input data (this is done in smaller batches)
- `batch_size` : When passed matrix or array data, the model slices the data into smaller batches and iterates over these batches during training. This integer specifies the size of each batch. Be aware that the last batch may be smaller if the total number of samples is not divisible by batch size. 
- `validation_data` : When prototyping a model, we want to easily monitor its performance on some validation data. Passing this argument - a list of inputs and labels - allows the model to display the loss and metrics in inference mode for the apssed data, at the end of each epoch.

```{r}
# example with validation data
data <- matrix(rnorm(100 * 32), nrow = 1000, ncol = 32)
labels <- matrix(rnorm(1000 * 10), nrow = 1000, ncol = 10)

val_data <- matrix(rnorm(1000 * 32), nrow = 100, ncol = 32)
val_labels <- matrix(rnorm(100 * 10), nrow = 100, ncol = 10)

model %>% fit(
              data,
              labels,
              epochs = 10,
              batch_size = 32,
              validation_data = list(val_data, val_labels)
)
```

#### Evaluate and Predict 

Same as fit, the evaluate and predict methods can use raw R data as well as a dataset. 

```{r}
# evaluate the inference mode loss and metrics for the data
## model %>% evaluate(test_data, test_labels, batch_size = 32)
## model %>% evaluate(test_dataset, steps = 30)

# predict
## model %>% predict(test_data, batch_size = 32)
## model %>% predict(test_dataset, steps = 30)
```

### Build Advanced Models 

#### Functional API

The Keras functional API is used to build complex model topologies, such as: 

- multi-input models
- multi-output models
- models with shared layers (the same layer called several times)
- models with non-sequential data flows (e.g., residual connections)

Building a model with the functional API works like this: 

1. A layer instance is callable and returns a tensor 
2. Input tensors and output tensors are used to define a keras_model instance
3. This model is trained just like the sequential model 

```{r}
# build a fully connected network with the functional api
inputs <- layer_input(shape = (32)) # return placeholder tensor

predictions <- inputs %>%
    layer_dense(units = 64, activation = "relu") %>%
    layer_dense(units = 64, activation = "relu") %>%
    layer_dense(units = 10, activation = "softmax")

# instantiate the model given inputs and outputs
model <- keras_model(inputs, outputs = predictions)

# compile step specifies the training configuration
model %>% compile(
              optimizer = optimizer_rmsprop(lr = 0.001),
              loss = "categorical_crossentropy",
              metrics = list("accuracy")
          )

model %>% fit(
              data,
              labels,
              batch_size = 32,
              epochs = 5
          )
```

#### Custom Layers

To create a custom layer, we can create an R6 class derived from `KerasLayer`. There are three methods to implement (only one of which, call(), is required for all types of layer). 

- `build(input_shape)` : This is where we will define our weights. Note that if our layer doesn't define trainable weights, then we need not implement this method. 

- `call(x)` : This is where the layers logic lives. Unless we want our layer to support masking, we only have to care about the first argument passed to call: the input tensor. 

- `compute_output_shape(input_shape)` : In case your layer modifies the shape of its input, we should specify here the shape transformation logic. This allows Keras to do automatic shape inference. If we don't modify the shape of the input, then we need not implement this method. 

Here is an example custom layer that performs a matrix multiplication:

```{r}
custom_layer <- R6::R6Class("CustomLayer",
                            inherit = KerasLayer,
                            public = list(
                                output_dim = NULL,

                                kernel = NULL,

                                initialize = function(output_dim) {
                                    self$output_dim <- output_dim
                                },

                                build = function(input_shape) {
                                    self$kernel <- self$add_weight(
                                                            name = 'kernel',
                                                            shape = list(input_shape[[2]], self$output_dim),
                                                            initializer = initializer_random_normal(),
                                                            trainable = TRUE)},

                                call = function(x, mask = NULL) {
                                    k_dot(x, self$kernel)
                                },

                                compute_output_shape = function(input_shape) {
                                    list(input_shape[[1]], self$output_dim)
                                }
                            )
                            )
```

In order to use the custom layer within a Keras model we will also need to create a wrapper function which instantiates the layer using the `create_layer` function. 

For example: 

```{r}
# define layer wrapper function
layer_custom <- function(object, output_dim, name = NULL, trainable = TRUE) {
    create_layer(custom_layer, object, list(
                                           output_dim = as.integer(output_dim),
                                           name = name,
                                           trainable = trainable
                                       ))
}

# use the custom layer in a model
model <- keras_model_sequential()

model %>%
    layer_dense(units = 32, input_shape = c(32, 32)) %>%
    layer_custom(output_dim = 32)
```

#### Custom Models 

In addition to creating custom layers, we can also create a custom model. This may be necessary if we wanted to use TensorFlow eager execution in combination with an imperatively written forward pass. In cases where this is not needed, but flexibility in building the architecture is required, it is recommended to stick with the functional API. 

A custom model is defined by called `keras_model_custom()` passing a function that specifies the layers to be created and the operations to be executed on forward pass.

```{r}
my_model <- function(input_dim, output_dim, name = NULL) {
    # define and return a custom model
    keras_model_custom(name = name, function(self) {
        # create layers we'll need for the call (executes once)
        # layers must be created on the self object
        self$dense1 <- layer_dense(units = 64, activation = "relu", input_shape = input_dim)
        self$dense2 <- layer_dense(units = 64, activation = "relu")
        self$dense3 <- layer_dense(units = 10, activation = "softmax")

        # implement call (executes during training and inference)
        function(inputs, mask = NULL) {
            x <- inputs %>%
                self$dense1() %>%
                self$dense2() %>%
                self$dense3()

            x
        }
    })
}

model <- my_model(input_dim = 32, output_dim = 10)

model %>% compile(
              optimizer = optimizer_rmsprop(lr = 0.001),
              loss = "categorical_crossentropy",
              metrics = list("accuracy")
          )

# train
model %>% fit(
              data,
              labels,
              batch_size = 32,
              epochs = 5              
)
```

#### Callbacks 

A callback is an object passed to a model to customize and extend its behavior during training. We can write our own custom callback, or use the builtin callbacks which include: 

- `callback_model_checkpoint` : Save checkpoints of our model at regular intervals 
- `callback_learning_rate_scheduler` : Dynamically change the learning rate 
- `callback_early_stopping` : Interrupt training when validation performance has stopped improving
- `callbacks_tensorboard` : Model the model's behavior using TensorBoard

To use a callback, pass it to the models fit method: 

```{r}
callbacks <- list(
    callback_early_stopping(patience = 2, monitor = "val_loss"),
    callback_tensorboard(log_dir = './logs')
)

model %>% fit(
              data,
              labels,
              batch_size = 32,
              epochs = 5,
              callbacks = callbacks,
              validation_data = list(val_data, val_labels)
          )
```

#### Save and Restore 

##### Weights Only 

```{r}
model %>% save_model_weights_hdf5("my_model.h5")

# restore model state
model %>% load_model_weights_hdf5("my_model.h5")
```

##### Configuration Only 

A model's configuration can be saved - this serializes the model architecture without any weights. A saved configuration can recreate and initialize the same model, even without the code that defined the original model. 

Keras supports JSON and YAML serialization formats:

```{r}
# custom models require different syntax
model <- keras_model_sequential()

model %>%
    layer_dense(units = 64, activation = "relu") %>%
    layer_dense(units = 64, activation = "relu") %>%
    layer_dense(units = 10, activation = "softmax")


model %>% compile(
              optimizer = "adam",
              loss = "categorical_crossentropy",
              metrics = list("accuracy")
          )

data <- matrix(rnorm(1000 * 32), nrow = 1000, ncol = 32)
labels <- matrix(rnorm(1000 * 10), nrow = 1000, ncol = 10)

model %>% fit(
              data,
              labels,
              epochs = 10,
              batch_size = 32
          )

# serialize a model to JSON format
json_string <- model %>% model_to_json()

# recreate the model (freshly initialized)
fresh_model <- model_from_json(json_string)

# serializes a model to YAML format
# yaml_string <- model %>% model_to_yaml()

# recreate the model
# fresh_model <- model_from_yaml()
```

Caution: Custom models are not serializable because their architecture is defined by the R code in the function passed to `keras_model_custom`. 

#### Entire Model

The entire model can be saved to a file that contains the weight values, model configuration, and even the optimizers configuration. This allows us to checkpoint a model and resume training later - from the exact state - without access to the original code.

```{r}
# save entire HDF5 file
# model %>% save_model_hdf5("my_model.h5")

# recreate the exact same model, including weights and optimizer
# model <- load_model_hdf5("my_model.h5")
```

#### Eager Execution 

Eager execution is an imperative programming environment that evaluates operations immediately. This is not required for keras, but it is supported by the Tensorflow backend and useful for inspecting our program and debugging. 

To use eager execution from R, we need to tell Keras to use the tensorflow implementation of Keras. 

```{r}
use_implementation("tensorflow")
```

When using the tensorflow implementation, all model building APIs are compatible with eager execution. 

#### Distribution

##### Estimators

The estimators API is used for training models for distributed environments. This targets industry use cases such as distributed training on large datasets that can export a model for production. 

Link : [tfestimators](https://tensorflow.rstudio.com/tfestimators/)

To use `tfestimators`, we need to install separately: 

```{r}
# install.packages("tfestimators", dependencies=T)
```

A model can be trained with the tfestimators API by converting the model to an estimator object with `keras_model_to_estimator`. Note: as of today, this only works with the Tensorflow implementation of Keras.

```{r}
library(tfestimators)

estimator <- keras_model_to_estimator(model)
```


## Sequential

The sequential model is a linear stack of layers. 

```{r}
model <- keras_model_sequential()

model %>%
    layer_dense(units = 32, input_shape = c(784)) %>%
    layer_activation("relu") %>%
    layer_dense(units = 10) %>%
    layer_activation("softmax")

model %>% summary()
```

### Input Shapes 
The model needs to know what input shape it should expect. For this reason, the first (and only the first) layer needs to receive information about its input shape. 

If you ever need to specify a fixed batch size for your inputs (for things like stateful recurrent networks), we can pass a batch_size argument to a layer. If we pass both batch_size = 32 and input_shape = c(6, 8) to a layer, it will then expect every batch of inputs to have the batch shape (32, 6, 8). 

### Compilation 

```{r}
# compile a model for multiclass classification
model <- keras_model_sequential()

model %>%
    layer_dense(units = 32, input_shape = c(784)) %>%
    layer_activation("relu") %>%
    layer_dense(units = 10) %>%
    layer_activation("softmax")

model %>% compile(
              optimizer = "rmsprop",
              loss = "categorical_crossentropy",
              metrics = c("accuracy")
          )
```

Here's what compilation might look like for a mean squared error regression problem: 

```{r}
model %>% compile(
              optimizer = optimizer_rmsprop(lr = 0.002),
              loss = "mse"
          )
```

Here's compilation for a binary classification problem: 

```{r}
model %>% compile(
              optimizer = optimizer_rmsprop(),
              loss = loss_binary_crossentropy,
              metrics = metric_binary_accuracy
          )
```

Here's compilation with a custom metric:

```{r}
# create metric used backend tensor functions
metric_mean_pred <- custom_metric("mean_pred", function(y_true, y_pred) {
    k_mean(y_pred)
})

model %>% compile(
              optimizer = optimizer_rmsprop(),
              loss = loss_binary_crossentropy,
              metrics = c("accuracy", metric_mean_pred)
          )
```

#### Training 

```{r}
# train a binary classifier

# create model
model <- keras_model_sequential()

# add layers and compile the model
model %>%
    layer_dense(units = 32, activation = "relu", input_shape = c(100)) %>%
    layer_dense(units = 1, activation = "sigmoid") %>%
    compile(
        optimizer = "rmsprop",
        loss = "binary_crossentropy",
        metrics = c("accuracy")
    )

# generate dummy data
data <- matrix(runif(1000 * 100), nrow = 1000, ncol = 100)
labels <- matrix(round(runif(1000, min = 0, max = 1)), nrow = 1000, ncol = 1)

# train the model, iterating on the data in batches of 32 samples
model %>% fit(data, labels, epochs = 10, batch_size = 32)
```

Here's a single input model with 10 classes (categorical classification): 

```{r}
# create model
model <- keras_model_sequential()

# define and compile model
model %>%
    layer_dense(units = 32, activation = "relu", input_shape = c(100)) %>%
    layer_dense(units = 10, activation = "softmax") %>%
    compile(
        optimizer = "rmsprop",
        loss = "categorical_crossentropy",
        metrics = c("accuracy")
    )

# generate dummy data
data <- matrix(runif(1000 * 100), nrow = 1000, ncol = 100)
labels <- matrix(round(runif(1000, min = 0, max = 9)), nrow = 1000, ncol = 1)

# convert labels to categorical one hot encoding
one_hot_labels <- to_categorical(labels, num_classes = 10)

# train the model, iterating on the data in batches of 32 samples
model %>% fit(data, one_hot_labels, epochs = 10, batch_size = 32)
```

### Examples 

#### MLP for multiclass softmax classification 

```{r}
# generate dummy data
x_train <- matrix(runif(1000*20), nrow = 1000, ncol = 20)

y_train <- runif(1000, min = 0, max = 9) %>%
    round() %>%
    matrix(nrow = 1000, ncol = 1) %>%
    to_categorical(num_classes = 10)

x_test <- matrix(runif(100*20), nrow = 100, ncol = 20)

y_test <- runif(100, min = 0, max = 9) %>%
    round() %>%
    matrix(nrow = 100, ncol = 1) %>%
    to_categorical(num_classes = 10)

# create model
model <- keras_model_sequential()

# define and compile the model
model %>%
    layer_dense(units = 64, activation = "relu", input_shape = c(20)) %>%
    layer_dropout(rate = 0.5) %>%
    layer_dense(units = 64, activation = "relu") %>%
    layer_dropout(rate = 0.5) %>%
    layer_dense(units = 10, activation = "softmax") %>%
    compile(
        loss = "categorical_crossentropy",
        optimizer = optimizer_sgd(lr = 0.01,
                                  decay = 1e-6,
                                  momentum = 0.9,
                                  nesterov = TRUE),
        metrics = c("accuracy")
    )

# train
model %>% fit(x_train, y_train, epochs = 20, batch_size = 128)

# evaluate
score <- model %>% evaluate(x_test, y_test, batch_size = 128)
```

#### MLP for Binary Classification 

```{r}
# generate dummy data
x_train <- matrix(runif(1000*20), nrow = 1000, ncol = 20)
y_train <- matrix(round(runif(1000, min = 0, max = 1)), nrow = 1000, ncol = 1)
x_test <- matrix(runif(100*20), nrow = 100, ncol = 20)
y_test <- matrix(round(runif(100, min = 0, max = 1)), nrow = 100, ncol = 1)

# create model
model <- keras_model_sequential()

# define and compile the model 
model %>%
    layer_dense(units = 64, activation = "relu", input_shape = c(20)) %>%
    layer_dropout(rate = 0.5) %>%
    layer_dense(units = 64, activation = "relu") %>%
    layer_dropout(rate = 0.5) %>%
    layer_dense(units = 1, activation = "sigmoid") %>% 
    compile(
        loss = "binary_crossentropy",
        optimizer = "rmsprop",
        metrics = c("accuracy")
    )

# train
model %>% fit(x_train, y_train, epochs = 20, batch_size = 128)

# evaluate
score <- model %>% evaluate(x_test, y_test, batch_size = 128)
```

#### VGG Like Convnet 

```{r}
# generate dummy data
x_train <- array(runif(100 * 100 * 100 * 3), dim = c(100, 100, 100, 3))
y_train <- runif(100, min = 0, max = 9) %>%
    round() %>%
    matrix(nrow = 100, ncol = 1) %>%
    to_categorical(num_classes = 10)
x_test <- array(runif(20 * 100 * 100 * 3), dim = c(20, 100, 100, 3))
y_test <- runif(20, min = 0, max = 9) %>% 
  round() %>%
  matrix(nrow = 20, ncol = 1) %>% 
  to_categorical(num_classes = 10)

# create model
model <- keras_model_sequential()

# define and compile
# in: 100x100 imgs w 3 channels -> (100, 100, 3) tensors
# applies 32 convolution filters 3x3 each
model %>%
    layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = "relu",
                  input_shape = c(100, 100, 3)) %>%
    layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu") %>%
    layer_max_pooling_2d(pool_size = c(2,2)) %>%
    layer_dropout(rate = 0.25) %>%
    layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>%
    layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>%
    layer_max_pooling_2d(pool_size = c(2,2)) %>%
    layer_dropout(rate = 0.25) %>%
    layer_flatten() %>%
    layer_dense(units = 256, activation = "relu") %>%
    layer_dropout(rate = 0.25) %>%
    layer_dense(units = 10, activation = "softmax") %>%
    compile(
        loss = "categorical_crossentropy",
        optimizer = optimizer_sgd(lr = 0.01,
                                  decay = 1e-6,
                                  momentum = 0.9,
                                  nesterov = TRUE)
)

# train
model %>% fit(x_train, y_train, batch_size = 32, epochs = 10) -> history

# evaluate
score <- model %>% evaluate(x_test, y_test, batch_size = 32)
```

#### Sequence Classification with LSTM 

```{r}
## model <- keras_model_sequential()

## model %>%
##     layer_embedding(input_dim = max_features, output_dim - 256) %>%
##     layer_lstm(units = 128) %>%
##     layer_dropout(rate = 0.5) %>%
##     layer_dense(units = 1, activation = "sigmoid") %>%
##     compile(
##         loss = "binary_crossentropy",
##         optimizer = "rmsprop",
##         metrics = c("accuracy")
##     )

## model %>% fit(x_train, y_train, batch_size = 16, epochs = 10)
## score <- model %>% evaluate(x_test, y_test, batch_size = 16)
```

#### Sequence Classification with 1D Convolutions 

```{r}
## model <- keras_model_sequential()

## model %>%
##     layer_conv_1d(filters = 64, kernel_size = 3, activation = "relu",
##                   input_shape = c(seq_length, 100)) %>%
##     layer_conv_1d(filters = 64, kernel_size = 3, activation = "relu") %>%
##     layer_max_pooling_1d(pool_size = 3) %>%
##     layer_conv_1d(filters = 128, kernel_size = 3, activation = "relu") %>%
##     layer_conv_1d(filters = 128, kernel_size = 3, activation = "relu") %>%
##     layer_global_average_pooling_1d() %>%
##     layer_dropout(rate = 0.5) %>%
##     layer_dense(units = 1, activation = "sigmoid") %>%
##     compile(
##         loss = "binary_crossentropy",
##         optimizer = "rmsprop",
##         metrics = c("accuracy")
##     )

## model %>% fit(x_train, y_train, batch_size = 16, epochs = 10)
## score <- model %>% evaluate(x_test, y_test, batch_size = 16)
```

#### Stacked LSTM for Sequence Classification 

In this model, we stack 3 LSTM layers on top of each other, making the model capable of learning higher level temporal representations. 

The first two LSTMs return their full output sequences, but the last one only returns the last step in its output sequence, thus dropping the temporal dimension (i.e. converting the input sequence into a single vector). 

![](stacked_lstm.png)

```{r}
# constants
data_dim <- 16
timesteps <- 8
num_classes <- 10

# define and compile model
model <- keras_model_sequential()

model %>%
    layer_lstm(units = 32, return_sequences = TRUE, input_shape = c(timesteps, data_dim)) %>%
    layer_lstm(units = 32, return_sequences = TRUE) %>%
    layer_lstm(units = 32) %>%
    layer_dense(units = 10, activation = "softmax") %>%
    compile(
        loss = "categorical_crossentropy",
        optimizer = "rmsprop",
        metrics = c("accuracy")
    )

# generate dummy training data
x_train <- array(runif(1000 * timesteps * data_dim), dim = c(1000, timesteps, data_dim))
y_train <- matrix(runif(1000 * num_classes), nrow = 1000, ncol = num_classes)

# generate dummy validation data
x_val <- array(runif(100 * timesteps * data_dim), dim = c(100, timesteps, data_dim))
y_val <- matrix(runif(100 * num_classes), nrow = 100, ncol = num_classes)

# train
model %>% fit(
              x_train, y_train, batch_size = 64, epochs = 5, validation_data = list(x_val, y_val)
)
```

#### Same Stacked LSTM model, rendered Stateful

A stateful recurrent model is one for which the internal states (memories) obtained after processing a batch of samples are reused for the samples of the next batch. This allows us to process longer sequences while keeping computational complexity manageable. 

```{r}
# constants
data_dim <- 16
timesteps <- 8
num_classes <- 10
batch_size <- 32

# define and compile model
# expects input batch shape (batch_size, timesteps, data_dim)
# note that we must provide the full batch_input_shape since the network is stateful. The sample of index i in batch k is the followup for the sample i in batch k-1
model <- keras_model_sequential()

model %>%
    layer_lstm(units = 32, return_sequences = TRUE, stateful = TRUE,
               batch_input_shape = c(batch_size, timesteps, data_dim)) %>%
    layer_lstm(units = 32, return_sequences = TRUE, stateful = TRUE) %>%
    layer_lstm(units = 10, activation = "softmax") %>%
    compile(
        loss = "categorical_crossentropy",
        optimizer = "rmsprop",
        metrics = c("accuracy")
    )

# generate dummy training data
x_train <- array(runif( (batch_size * 10) * timesteps * data_dim), 
                 dim = c(batch_size * 10, timesteps, data_dim))
y_train <- matrix(runif( (batch_size * 10) * num_classes), 
                  nrow = batch_size * 10, ncol = num_classes)

# generate dummy validation data
x_val <- array(runif( (batch_size * 3) * timesteps * data_dim), 
               dim = c(batch_size * 3, timesteps, data_dim))
y_val <- matrix(runif( (batch_size * 3) * num_classes), 
                nrow = batch_size * 3, ncol = num_classes)

# train
model %>% fit(
              x_train,
              y_train,
              batch_size = batch_size,
              epochs = 5,
              shuffle = FALSE,
              validation_data = list(x_val, y_val)
          )

```

## Functional 

The Keras functional API is the way to go for defining complex models, such as multi output models, directed acyclic graphs, or models with shared layers. 

### First Example: A Densely Connected Network

The sequential model is a better choice for this example. To use the functional API, build your input and output layers and then pass them to the model() function. 

```{r}
# input layer
inputs <- layer_input(shape = c(784))

# outputs compose input + dense layers
predictions <- inputs %>%
    layer_dense(units = 64, activation = "relu") %>%
    layer_dense(units = 64, activation = "relu") %>%
    layer_dense(units = 10, activation = "softmax")

# create and compile model
model <- keras_model(inputs, predictions)

model %>% compile(
              optimizer = "rmsprop",
              loss = "categorical_crossentropy",
              metrics = c("accuracy")
          )

```
Note that Keras objects are modified in place, which is why its not necessary for model to be assigned back after it is compiled. 

#### All models are callable, just like layers 

With the functional API, its easy to reuse trained models. you can treat any model as if it were a layer. Note that we aren't just reusing the architecture, but also the weights. 

```{r}
x <- layer_input(shape = c(784))

# this works and returns the 10way softmax defined above
y <- x %>% model
```

This can allow us to quickly create models that can process sequences of inputs. We could turn an image classification model into a video classification model in just one line. 

```{r}
# input tensor for sequences of 20 timesteps each containing a 784D vector
input_sequences <- layer_input(shape = c(20, 784))

# this applies our previous model to the input sequence
processed_sequences <- input_sequences %>%
    time_distributed(model)
```

### Multi-Input and Multi-Output Models 

The functional API makes it easy to manipulate a large number of intertwined data streams. 

Consider the following model: Suppose we wish to predict how many retweets and likes a news headline will receive on twitter. The main input to the model is the headline itself, as a sequence of words. As an auxilliary input, we will receive data such as time of day when the headline was posted. 

The model will also be supervised via two loss functions. Using the main loss function earlier in the model is a good regularization mechanism for deep models. 

Here is a diagram: 

![](mimo.png)

The main input will receive the headline as a sequence of integers (each integer encodes a word). The integers will be in [1, 10000] and the sequences will be 100 words long. 

```{r}
main_input <- layer_input(shape = c(100), dtype = "int32", name = "main_input")

lstm_out <- main_input %>%
    layer_embedding(input_dim = 10000, output_dim = 512, input_length = 100) %>%
    layer_lstm(units = 32)

# insert aux loss
# allows the lstm and embedding layers to be trained smoothly
auxiliary_output <- lstm_out %>%
    layer_dense(units = 1, activation = "sigmoid", name = "aux_output")

# feed aux into the model by concatenation & stack a deep densely connected network on top adding a logistic regression layer
auxiliary_input <- layer_input(shape = c(5), name = "aux_input")

# combine 
main_output <- layer_concatenate(c(lstm_out, auxiliary_input)) %>%
    layer_dense(units = 64, activation = "relu") %>%
    layer_dense(units = 64, activation = "relu") %>%
    layer_dense(units = 64, activation = "relu") %>%
    layer_dense(units = 1, activation = "sigmoid", name = "main_output")

# define a model with two inputs and two outputs
model <- keras_model(
    inputs = c(main_input, auxiliary_input),
    outputs = c(main_output, auxiliary_output)
)

model %>% summary()
```

We compile the model and assign a weight of 0.2 to the auxiliary loss. To specify different loss_weights or loss for each different output, we can use a list or a dictionary. Here we pass a single loss as the loss argument, so the same loss will be used on all outputs. 

```{r}
model %>% compile(
              optimizer = "rmsprop",
              loss = "binary_crossentropy",
              loss_weights = c(1.0, 0.2)
          )

# train the model
## model %>% fit(
##               x = list(headline_data, additional_data),
##               y = list(labels, labels),
##               epochs = 50,
##               batch_size = 32
##           )

```

Since our inputs and outputs are named, we could also have compiled the model via 

```{r}
model %>% compile(
              optimizer = "rmsprop",
              loss = list(main_output = 'binary_crossentropy',
                          aux_output = 'binary_crossentropy'),
              loss_weights = list(main_output = 1.0, aux_output = 0.2)
          )

# train
## model %>% fit(
##               x = list(main_input = headline_data, aux_input = additional_data),
##               y = list(main_output = labels, aux_output = labels),
##               epochs = 50,
##               batch_size = 32
##           )
```

### Shared Layers 

Consider a dataset of tweets. We wish to build a model that can tell whether two tweets are from the same person or not (this can allow us to compare users by the similarity of their tweets for instance). 

One way to achieve this is to build a model that encodes two tweets into two vectors, concatenates the vectors and then adds a logistic regression. This outputs a probability that the two tweets share the same author. This model would then be trained on positive tweet pairs and negative tweet pairs. 

Since the problem is symmetric, the mechanism that encodes the first tweet should be reused (weights and all) to encode the second tweet. Here we use a shared LSTM layer to encode the tweets. 

Let's build this with the functional API. We will take as input for a tweet a binary matrix of shape (280, 256), where each dimension in the vector encodes the presence / absence of a character (out of an alphabet of 256 frequent characters).

```{r}
tweet_a <- layer_input(shape = c(280, 256))
tweet_b <- layer_input(shape = c(280, 256))
```

To share a layer across different inputs, simply instantiate the layer once, then call it on as many inputs as you want. 

```{r}
# layer can take as input a matrix and will return a vector of size 64
shared_lstm <- layer_lstm(units = 64)

# when we reuse a layer many times, the weights are being reused
encoded_a <- tweet_a %>% shared_lstm
encoded_b <- tweet_b %>% shared_lstm

# we can then concatenate the two vectors and add a log reg on top
predictions <- layer_concatenate(c(encoded_a, encoded_b), axis = -1) %>%
    layer_dense(units = 1, activation = "sigmoid")

# define a trainable model linking the tweet inputs to predictions
model <- keras_model(inputs = c(tweet_a, tweet_b), outputs = predictions)

model %>% compile(
              optimizer = "rmsprop",
              loss = "binary_crossentropy",
              metrics = c("accuracy")
          )

# model %>% fit(list(data_a, data_b), labels, epochs = 10)
```

### The Concept of a Layer Node 

Whenever you call a layer on some input, you create a new tensor and you add a node to the layer, linking the input tensor to the output tensor. When you are calling the same layer multiple times, the layer owns multiple nodes indexed as 1,2,2,...

We can obtain the output tensor of a layer via `layer$output`, or its output shape via `layer$output_shape`. 

As long as a layer is only connected to one input, there is no confusion and `\$output` will return one output of the layer. 

```{r}
a <- layer_input(shape = c(280, 256))

lstm <- layer_lstm(units = 32)

encoded_d <- a %>% lstm

lstm$output
```

Not so if the layer has multiple inputs: 

```{r}
a <- layer_input(shape = c(280, 256))
b <- layer_input(shape = c(280, 256))

lstm <- layer_lstm(units = 32)

encoded_a <- a %>% lstm
encoded_b <- b %>% lstm 

lstm$output
```

The tutorial shows the following output : 

`AttributeError: Layer lstm_4 has multiple inbound nodes, hence the notion of "layer output" is ill-defined. Use `get_output_at(node_index)` instead.`

This feature may have been changed in the time between. 

```{r}
get_output_at(lstm, 1)
get_output_at(lstm, 2)
```

### More Examples 

#### Inception Module 

For more information about the inception architecture, see [Going Deeper with Convolutions](https://arxiv.org/abs/1409.4842)

##### Abstract

We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.

![](imagenet_naive.png)

```{r}
input_img <- layer_input(shape = c(256, 256, 3))

tower_1 <- input_img %>%
    layer_conv_2d(filters = 64, kernel_size = c(1, 1), padding = "same", activation = "relu") %>%
    layer_conv_2d(filters = 64, kernel_size = c(3, 3), padding = "same", activation = "relu")

tower_2 <- input_img %>%
    layer_conv_2d(filters = 64, kernel_size = c(1, 1), padding = "same", activation = "relu") %>%
    layer_conv_2d(filters = 64, kernel_size = c(5, 5), padding = "same", activation = "relu")

tower_3 <- input_img %>%
    layer_max_pooling_2d(pool_size = c(3, 3), strides = c(1, 1), padding = "same") %>%
    layer_conv_2d(filters = 64, kernel_size = c(1, 1), padding = "same", activation = "relu")

output <- layer_concatenate(c(tower_1, tower_2, tower_3), axis = 1)
```

#### Residual Connection on a Convolution Layer 

For more information, see [Deep Residual Learning for Image Recognition](arxiv.org/abs/1512.03385)

##### Abstract 

Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. 

The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.

![](residual_block.png)

```{r}
# input tensor for a 3 channel 256x256 image
x <- layer_input(shape = c(256, 256, 3))

# 3x3 conv with 3 output channels (same as input)
y <- x %>% layer_conv_2d(filters = 3, kernel_size = c(3, 3), padding = "same")

# this returns x + y
z <- layer_add(c(x, y))
```

#### Shared Vision Model

This model reuses the same image processing module on two inputs to classify whether two MNIST digits are the same digit or different digits. 

```{r}
# first define the vision model
digit_input <- layer_input(shape = c(27, 27, 1))

out <- digit_input %>%
    layer_conv_2d(filters = 64, kernel_size = c(3, 3)) %>%
    layer_conv_2d(filters = 64, kernel_size = c(3, 3)) %>%
    layer_max_pooling_2d(pool_size = c(2, 2)) %>%
    layer_flatten()

vision_model <- keras_model(digit_input, out)

# define the tell-digits-apart model
digit_a <- layer_input(shape = c(27, 27, 1))
digit_b <- layer_input(shape = c(27, 27, 1))

# the vision model will be shared, weights and all
out_a <- digit_a %>% vision_model
out_b <- digit_b %>% vision_model

out <- layer_concatenate(c(out_a, out_b)) %>%
    layer_dense(units = 1, activation = "sigmoid")

classification_model <- keras_model(inputs = c(digit_a, digit_b), out)
```

#### Visual Question Answering Model 

This model can select the correct one word answer when asked a natural language question about a picture. 

It works by encoding the question into a vector, encoding the image into a vector, concatenating the two, and training on top a logistic regression over some vocabulary of potential answers. 

```{r}
# define a vision model using a sequential model

# encode image into a vector
vision_model <- keras_model_sequential()

vision_model %>%
    layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu",
                  padding = "same", input_shape = c(224, 224, 3)) %>%
    layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>%
    layer_max_pooling_2d(pool_size = c(2, 2)) %>%
    layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu", padding = "same") %>%
    layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>%
    layer_max_pooling_2d(pool_size = c(2, 2)) %>%
    layer_conv_2d(filters = 256, kernel_size = c(3, 3), activation = "relu", padding = "same") %>%
    layer_conv_2d(filters = 256, kernel_size = c(3, 3), activation = "relu") %>%
    layer_conv_2d(filters = 256, kernel_size = c(3, 3), activation = "relu") %>%
    layer_max_pooling_2d(pool_size = c(2, 2)) %>%
    layer_flatten()

# get a tensor with the output of our vision model
image_input <- layer_input(shape = c(224, 224, 3))
encoded_image <- image_input %>% vision_model

# define a language model to encode the question into a vector
# each question <= 100 words and indexed in [1, 9999]
question_input <- layer_input(shape = c(100), dtype = "int32")
encoded_question <- question_input %>%
    layer_embedding(input_dim = 10000, output_dim = 256, input_length = 100) %>%
    layer_lstm(units = 256)

# concatenate the question and image vectors then train a log reg on top
output <- layer_concatenate(c(encoded_question, encoded_image)) %>%
    layer_dense(units = 1000, activation = "softmax")

# final model 
vqa_model <- keras_model(inputs = c(image_input, question_input), outputs = output)
```

#### Video Question Answering Model 

Now that we have our image QA model, we can quickly turn it into a video QA model. With appropriate training, we can show our model a short video (e.g. a 100 frame human action) and ask a natural language question about the video. 

```{r}
video_input <- layer_input(shape = c(100, 224, 224, 3))

# this is our video encoded via the previous vision model (weights reused)
encoded_video <- video_input %>%
    time_distributed(vision_model) %>%
    layer_lstm(units = 256)

# model level representation of the question encoder, reusing weights
question_encoder <- keras_model(inputs = question_input, outputs = encoded_question)

# encode question
video_question_input <- layer_input(shape = c(100), dtype = "int32")
encoded_video_question <- video_question_input %>% question_encoder

# create video question answering model
output <- layer_concatenate(c(encoded_video, encoded_video_question)) %>%
    layer_dense(units = 1000, activation = "softmax")

video_qa_model <- keras_model(inputs = c(video_input, video_question_input), outputs = output)
```

## Keras Models 

Keras supports 3 types of models:

- sequential
- functional api
- custom

### Sequential

```{r}
model <- keras_model_sequential()

model %>%
    layer_dense(units = 32, input_shape = c(784)) %>%
    layer_activation('relu') %>%
    layer_dense(units = 10) %>%
    layer_activation('softmax')
```

Note that keras objects are modified in place, which is why assignment is not necessary. 

### Functional

The functional API allows for more complex models, such as multi output models, directed acyclic graphs, and shared layers. To create a model with the functional API compose a set of input and output layers and then pass them to the `keras_model()` function. 

```{r}
# define two inputs
tweet_a <- layer_input(shape = c(140, 256))
tweet_b <- layer_input(shape = c(140, 256))

# take input as a matrix and return a vector of 64 units
shared_lstm <- layer_lstm(units = 64)

# using the same layer instance multiple times reuses weights
encoded_a <- tweet_a %>% shared_lstm
encoded_b <- tweet_b %>% shared_lstm

# concatenate two vectors and add log regression
predictions <- layer_concatenate(c(encoded_a, encoded_b), axis = -1) %>%
    layer_dense(units = 1, activation = 'sigmoid')

# define a trainable model linking inputs to predictions
model <- keras_model(inputs = c(tweet_a, tweet_b), outputs = predictions)
```

### Custom 

Custom models enable the implementation of custom forward pass logic (i.e. to encapsulte the logic associated with constructing various types of models).

All models share the following properties: 

- model$layers - a flattened list of the layers comprising the model graph 
- model$inputs - list of input tensors 
- model$outputs - list of output tensors 

## Layers 

A wide variety of layers are available, including: 

- core layers
- convolutional layers
- pooling layers
- activation layers
- dropout layers
- locally connected layers
- recurrent layers
- embedding layers
- normalization layers
- noise layers 
- merge layers
- layer wrappers

## Visualization 

There are a number of tools for visualizing the training of keras models, including: 

- A plot method
- the RStudio IDE
- integration with tensorboard 
  - tensorboard provides other visualizations like: 
    - the underlying tensorflow graph
    - gradient histograms
    - model weights
  - enables comparison of metrics across multiple training runs

### Plotting History 

The history will be plotted using ggplot2 if available. 

If you want to create a custom visualization, you can call the `as.data.frame()` method on the history to obtain a data frame with factors for each metric as well as training vs validation: 

```{r}
## history_df <- as.data.frame(history)
## str(history_df)
```

### Tensorboard

To record data that can be visualized with tensorboard, we can add a tensorboard callback to the fit() function. For example:

```{r}
## history <- model %>% fit(
##                          x_train, y_train,
##                          batch_size = batch_size,
##                          epochs = epochs,
##                          verbose = 1,
##                          callbacks = callback_tensorboard("logs/run_a"),
##                          validation_split = 0.2
##                      )
```

The the docs on callback_tensorboard() for more info. The important part is the logs directiory. A distinct log direction should be used for each training run. 

#### Viewing Data 

To view tensorboard data for a given set of runs, we use the tensorboard() function, pointing it to the previously specific `log_dir`: 

```{r}
# tensorboard("logs/run_a")
```

It is often useful to run tensorboard while we train a model. 

```{r}
# launch tensorboard
# tensorboard("logs/run_a")

## # set epochs
## epochs <- 10

## # fit the model with the tensorboard callback
## history <- model %>% fit(
##                          x_train, y_train,
##                          batch_size = batch_size,
##                          epochs = epochs,
##                          verbose = 1,
##                          callbacks = callback_tensorboard("logs/run_a"),
##                          validation_split = 0.2
##                      )
```

Keras writes tensorboard data at the end of each epoch. 

#### Comparing Runs 

TensorBoard will automatically include all runs logged within the subdirectories of the specified log_dir. 

We can also pass multiple directories: 

```{r}
# log another run
# callback_tensorboard(log_dir = "logs/run_b")

# call tensorboard
# tensorboard("logs")

# multiple log directories
# tensorboard(c("logs/run_a", "logs/run_b"))
```

#### Customization 

Metrics: 

The tensorboard callback will log data for any metrics which are specified in the metrics parameter of the compile function. 

```{r}
# example
model %>% compile(
              loss = 'mean_squared_error',
              optimizer = 'sgd',
              metrics = c('mae', 'acc')
          )
```

tensorboard data series will be created for the loss (mean squared error), as well as for the mean absolute error and accuracy metrics. 

## Pretrained

Keras Applications are deep learning models that are made available alongside pretrained weights. 

The following image classification models are available: 

- xception
- VGG16
- VGG19
- ResNet50
- InceptionV3
- InceptionResNetV2
- MobileNet
- MobileNetV2
- DenseNet
- NASNet

### Usage Examples 

#### Classify Imagenet with ResNet50

```{r}
## # instantiate the model
## model <- application_resnet50(weights = 'imagenet')

## # load the image
## img_path <- "elephant.jpg"
## img <- image_load(img_path, target_size = c(224, 224))
## x <- image_to_array(img)

## # ensure we have a 4d tensor with single element in the batch dimension, then preprocess the input for prediction using resnet50
## x <- array_reshape(x, c(1, dim(x)))
## x <- imagenet_preprocess_input(x)

## # make predictions then decode and print them
## preds <- model %>% predict(x)
## imagenet_decode_predictions(preds, top = 3)[[1]]
```

#### Extract Features with VGG16

```{r}
## model <- application_vgg16(weights = 'imagenet', include_top = FALSE)

## img_path <- "elephant.jpg"
## img <- image_load(img_path, target_size = c(224, 224))
## x <- image_to_array(img)

## x <- array_reshape(x, c(1, dim(x)))
## x <- imagenet_preprocess_input(x)

## features <- model %>% predict(x) 
# imagenet_decode_predictions(features, top = 3)[[1]]
```
    
#### Extract features from an arbitrary intermediate layer with VGG19 

```{r}
## base_model <- application_vgg19(weights = "imagenet")

## model <- keras_model(inputs = base_model$input,
##                      outputs = get_layer(base_model, 'block4_pool')$output)

## img_path <- "elephant.jpg"
## img <- image_load(img_path, target_size = c(224, 224))
## x <- image_to_array(img)
## x <- array_reshape(x, c(1, dim(x)))
## x <- imagenet_preprocess_input(x)

## block4_pool_features <- model %>% predict(x)

## imagenet_decode_predictions(block4_pool_features, top = 3)[[1]]
```

#### Fine Tune InceptionV3 on a new set of classes

```{r}
## # create the base pretrained model
## base_model <- application_inception_v3(weights = 'imagenet', include_top = FALSE)

## # add our custom layers
## predictions <- base_model$output %>%
##     layer_global_average_pooling_2d() %>%
##     layer_dense(units = 1024, activation = 'relu') %>%
##     layer_dense(units = 200, activation = 'softmax')

## # create training model
## model <- keras_model(inputs = base_model$input, outputs = predictions)

## # freeze all convolutional inceptionv3 layers
## freeze_weights(base_model)

## # compile the model
## model %>% compile(
##               optimizer = "rmsprop",
##               loss = "categorical_crossentropy"
##           )

## # train the model on a few epochs
## model %>% fit_generator(...)

## # at this point, the top layers are well trained and we can start fine tuning convolutional layers from inception V3. We will freeze the bottom N layers and train the remaining top layers.

## # visualize layer names and indices to see what we should freeze
## layers <- base_model$layers

## for (i in 1:length(layers)) cat(i, layers[[i]]$name, "\n")

## # train the top 2 inception blocks (freeze first 172 layers)
## freeze_weights(base_model, from = 1, to = 172)
## unfreeze_weights(base_model, from = 173)

## # recompile the model for these modifications to take effect
## model %>% compile(
##               optimizer = optimizer_sgd(lr = 0.0001, momentum = 0.9),
##               loss = "categorical_crossentropy"
##           )

## # train the model again fine tuning the top 2 inception blocks
## model %>% fit_generator(...)
```

#### Build InceptionV3 over a custom input tensor 

```{r}
# this could also be the output to a different keras model or layer
## input_tensor <- layer_input(shape = c(224, 224, 3))

## model <- application_inception_V3(input_tensor = input_tensor,
##                                   weights = "imagenet",
##                                   include_top = TRUE)
```

## FAQ

### How can I use Keras with datasets that don't fit in memory? 

#### Generator Functions

```{r}
# simple generator function example
## sampling_generator <- function(x_data, y_data, batch_size) {
##     function() {
##         rows <- sample(1:nrow(x_data), batch_size, replace = TRUE)
##         list(x_data[rows, ], y_data[rows,])
##     }
## }

## model %>%
##     fit_generator(sampling_generator(x_train, y_train, batch_size = 128),
##                   steps_per_epoch = nrow(x_train) / 128, epochs = 10)
```

The `steps_per_epoch` parameter indicates the number of steps (batches of samples) to yield from the generator before declaring one epoch finished and starting the next epoch. It should typically be equal to the number of unique samples if your dataset is divided by the batch size. 

#### External Data Generators 

The above example doesn't address the use case of datasets that don't fit in memory. Typically, to do that we will need to write a generator that reads from another source (e.g. a sparse matrix or files on disk) and maintains an offset into that data as its called repeatedly. 

For example, suppose we have a set of text files we wish to read from: 

```{r}
data_files_generator <- function(dir) {
    files <- list.files(dir)
    next_file <- 0

    function() {
        # move to the next file (note <<- scoping assignment)
        next_file <<- next_file + 1

        # if all files exhausted start at beginning
        # keras generators need to yield indefinitely
        # termination is controlled by the epochs / steps_per_epoch
        if (next_file > length(files)){
            next_file <<- 1
        }
        
        # determine the file name
        file <- files[[next_file]]

        # process and return the data in the file
        # in a real example subdivide the data within the file into appropriately sized training batches.
        file_to_training_data(file)
    }
}
```

The above code is an example of a stateful generator - the function maintains information across calls to keep track of which data to provide next. 

#### Image Generators 

We can also use the

- `flow_images_from_directory`
- `flow_images_from_data`

functions along with `fit_generator` for training on sets of images stored on disk (with optional image augmentation / normalization via `image_data_generator`). 

We can see batch image training in action in the [cifar10 example](https://keras.rstudio.com/articles/examples/cifar10_cnn.html). 

#### Batch Functions 

We can also use the `train_on_batch` and `test_on_batch` functions, which enable us to write training loops that read into memory only the data required for each batch. 

### Interrupt Training

We can use early stopping callbacks to interrupt training when the validation loss isn't decreasing anymore. 

```{r}
early_stopping <- callback_early_stopping(monitor = 'val_loss', patience = 2)

# model %>% fit(x, y, validation_split = 0.2, callbacks = c(early_stopping))
```

### Freezing Layers

To freeze a layer means to exlude it from training, i.e. its weights will never be updated. This is useful in the context of fine tuning a model, or using fixed embedings for a text input. 

```{r}
## # pass a trainable arg to a layer constructor
## frozen_layer <- layer_dense(units = 32, trainable = FALSE)

## # OR set the trainable property after instantiation
## x <- layer_input(shape = c(32))
## layer <- layer_dense(units = 32)
## layer$trainable <- FALSE
## y <- x %>% layer

## frozen_model <- keras_model(x, y)
## frozen_model %>% compile(optimizer = "rmsprop", loss = "mse")

## layer$trainable <- TRUE
## trainable_model <- keras_model(x, y)
## trainable_model %>% compile(optimizer = "rmsprop", loss = "mse")

## frozen_model %>% fit(data, labels) # don't update weights
## trainable_model %>% fit(data, labels) # this does update weights
```

We can freeze or unfreeze the weights for an entire model using the `freeze_weights` and `unfreeze_weights` functions. For example: 

```{r}
# instantiate a VGG16 model
conv_base <- application_vgg16(
    weights = "imagenet",
    include_top = FALSE,
    input_shape = c(150, 150, 3)
)

# freeze its weights
freeze_weights(conv_base)

# create a composite model with base + more layers
model <- keras_model_sequential() %>%
    conv_base %>%
    layer_flatten() %>%
    layer_dense(units = 256, activation = "relu") %>%
    layer_dense(units = 1, activation = "sigmoid")

# compile
model %>% compile(
              loss = "binary_crossentropy",
              optimizer = optimizer_rmsprop(lr = 2e-5),
              metrics = c("accuracy")
          )

# unfreeze weights from block5_conv1 on
unfreeze_weights(conv_base, from = "block5_conv1")

# compile again since we froze or unfroze layers
model %>% compile(
              loss = "binary_crossentropy",
              optimizer = optimizer_rmsprop(lr = 2e-5),
              metrics = c("accuracy")
          )
```

### Stateful RNNs
Making a RNN stateful means that the states for the samples of each batch will be reused as initial states for the samples in the next batch. 

When using stateful RNNs, it is assumed that: 

- all batches have the same number of samples 
- If x1 and x2 are successive batches of samples, then x2[[i]] is the follow up sequence to x1[[i]] for every i

To use statefulness in RNNs, we must:

- explicitly specify the batch size we are using, by passing a batch size argument to the first layer in our model, e.g. `batch_size = 32` for a 32 samples batch of 10 timesteps with 16 features per timestep. 
- set `stateful = TRUE` in our RNN layer 
- specify `shuffle = FALSE` when calling fit 

To reset the states accumulated in either a single layer or an entire model use the `reset_states()` function. 

### Remove a Layer

We can remove the last added layer in a sequential model by calling `pop_layer`

```{r}
model <- keras_model_sequential()

model %>%
    layer_dense(units = 32, activation = "relu", input_shape = c(784)) %>%
    layer_dense(units = 32, activation = "relu") %>%
    layer_dense(units = 32, activation = "relu")

length(model$layers)

model %>% pop_layer()

length(model$layers)
```

### Pretrained Models 

Code and pre-trained weights are available for the following image classification models: 

Xception
VGG16
VGG19
ResNet50
InceptionV3
InceptionResNetV2
MobileNet
MobileNetV2
DenseNet
NASNet

### Accessing the underlying python

```{r}
# keras python module
keras <- NULL

# obtain a reference to the module from the keras R package
.onLoad <- function(libname, pkgname) {
    keras <<- keras::implementation()
}
```

### Reproducibility 

The `use_session_with_seed` function establishes a common random seed for R, python, numpy, and tensorflow. It furthermore disables hash randomization, GPU computations, and CPU parallelization, which can be additional sources of non-reproducibility.

```{r}
# call directly after loading package
# library(keras)
# use_session_with_seed(88)

# or don't disable GPU or CPU parallelization
# use_session_with_seed(888, disable_gpu = FALSE, disable_parallel_cpu = FALSE)
```

### Deployability 

On iOS, via Apple’s CoreML (Keras support officially provided by Apple)
On Android, via the TensorFlow Android runtime. Example: Not Hotdog app
In the browser, via GPU-accelerated JavaScript runtimes such as Keras.js and WebDNN
On Google Cloud, via TensorFlow-Serving
In an R or Python webapp backend (such as a Shiny or Flask app)

## Eager 

Eager execution is a way to train a keras model without building a graph. Operations return values, not tensors. Consequently, we can inspect what goes in and comes out of an operation by simply printing a variable's contents. 


```{r}
library(keras)
library(tensorflow)
tfe_enable_eager_execution(device_policy = "silent")

# check if we are using eager execution
tf$executing_eagerly()
```

### Define a Model

Models for use with eager execution are defined as Keras custom models. 

```{r}
# model instantiator
iris_regression_model <- function(name = NULL) {
    keras_model_custom(name = name, function(self) {
        # define any number of layers here
        self$dense1 <- layer_dense(units = 32)
        self$dropout <- layer_dropout(rate = 0.5)
        self$dense2 <- layer_dense(units = 1)

        # this is the call fn that defines what happens when the model is called
        function(x, mask = NULL) {
            x %>%
                self$dense1() %>%
                self$dropout() %>%
                self$dense2()
        }
    })
}

# create the model by instantiation via its wrapper
model <- iris_regression_model()

# call the model on dummy data even though its weights are still unknown
model(k_constant(matrix(1:6, nrow = 2, ncol = 3)))

# inspect the models weights
model$weights
```

### Losses and Optimizers 

An appropriate loss function for regression is the mean squared error 

```{r}
mse_loss <- function(y_true, y_pred, x) {
    # its reqd to use a TF function here
    mse <- tf$losses$mean_squared_error(y_true, y_pred)

    # here we could compute and add other losses
    mse
}

# in the same view, we need an optimizer from the tf$train module
optimizer <- tf$train$AdamOptimizer()
```

### Use tfdatasets to feed the data 

In eager execution we use tfdatasets to stream input and target data to the model. In this example, we use `tensor_slices_dataset` to directly create a dataset from the underlying R matrices x_train and y_train. 

A wide variety of other dataset creation functions are available: [dataset creation](https://tensorflow.rstudio.com/tools/tfdatasets/reference/#section-creating-datasets)

Datasets also allow for a variety of preprocessing operations:

```{r}
x_train <- iris[1:120, c("Petal.Length", "Sepal.Length", "Petal.Width")] %>%
    as.matrix()
y_train <- iris[1:120, c("Sepal.Width")] %>% as.matrix()

# convert to appropriate tensor floats type for backend
x_train <- k_constant(x_train)
y_train <- k_constant(y_train)

# same for test
x_test <- iris[121:150, c("Petal.Length", "Sepal.Length", "Petal.Width")] %>% as.matrix()
y_test <- iris[121:150, c("Sepal.Width")] %>% as.matrix()
x_test <- k_constant(x_test)
y_test <- k_constant(y_test)

library(tfdatasets)

train_dataset <- tensor_slices_dataset(list(x_train, y_train)) %>%
    dataset_batch(10)
test_dataset <- tensor_slices_dataset(list(x_train, y_train)) %>%
    dataset_batch(10)


```

## Callbacks

