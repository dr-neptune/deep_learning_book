\message{ !name(ch9_convnets.tex)}\documentclass[onecolumn, letterpaper, 12pt]{report}

% insert any packages, environments, counters, etc here
\usepackage{graphicx}
\usepackage{hyperref}

\begin{document}

\message{ !name(ch9_convnets.tex) !offset(211) }
We propose a deep convolutional neural network architecture
  codenamed Inception, which was responsible for setting the new state
  of the art for classification and detection in the ImageNet
  Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main
  hallmark of this architecture is the improved utilization of the
  computing resources inside the network. This was achieved by a
  carefully crafted design that allows for increasing the depth and
  width of the network while keeping the computational budget
  constant. To optimize quality, the architectural decisions were
  based on the Hebbian principle and the intuition of multi-scale
  processing. One particular incarnation used in our submission for
  ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality
  of which is assessed in the context of classification and detection.
\end{quote}

\section{Variants of the Basic Convolution Function}

Assume we have a 4D kernel tensor $K$ with element $K_{i, j, k, l}$ giving the connection strength between a unit in channel i of the output and a unit in channel j of the input, with an offset of k rows and l columns between the output unit and the input unit. Assume our input consists of observed data $V$ with element $V_{i,j,k}$ giving the value of the input unit within channel i at row j and column k. Assume our output consists of $Z$ with the same format as $V$. If $Z$ is produced by convolving $K$ across $V$ without flipping $K$, then 

\begin{center}
  $Z_{i, j, k} = \sum\limits_{l,m,n} V_{l, j+m-1}K_{i,l,m,n}$
\end{center}

We may wish to skip over some of the positions of the kernel in order to reduce the computational cost (at expense). We can think of this as downsampling the output of the full convolution function. If we wish to sample only every $s$ pixels in each direction in the output, then we can define a downsampled convolution function c such that 

\begin{center}
  $Z_{i, j, k} = c(K, V, s)_{i, j, k} = \sum\limits_{l, m, n}[V_{l, (j-1) \times s + m, (k-1) \times s+n}K_{i, l, m, n}]$
\message{ !name(ch9_convnets.tex) !offset(211) }

\end{document}
